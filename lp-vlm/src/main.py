import sys,os
import json
import subprocess
import os
import time
import queue
from collections import defaultdict
import threading
import time

from utils.config import (SAMPLE_MEDIA_DIR,
                          RESULTS_DIR,
                          LP_APP_BASE_DIR,
                          
                          LP_IP,MINIO_API_HOST_PORT, 
                          COMMON_RESULTS_DIR_FULL_PATH,
                          )
from utils.vlm import call_vlm
from utils.frames_processor import get_best_frame
from agent.agent import ConfigAgent
import re
from utils.save_results import get_presigned_url
from utils.config import logger,INVENTORY_FILE
from utils.rabbitmq_consumer import ODConsumer
import traceback
from workload_utils import get_video_from_config

# ============================================================================
# GLOBAL VARIABLES AND QUEUES
# ============================================================================
BOLD = "\033[1m"
UNDERLINE = "\033[4m"
RESET = "\033[0m"
CYAN = "\033[34m"
RABBITMQ_USER = os.environ.get("RABBITMQ_USER")
RABBITMQ_PASSWORD = os.environ.get("RABBITMQ_PASSWORD")

vlm_queue = queue.Queue()
result_queue = queue.Queue()
od_message_queue = queue.Queue()
inventory_list = None
inventory_set = None

# ============================================================================
# Helper Function
# ============================================================================
def write_json_to_file(data, file_path):
    """
    Append a dict as a new line to a JSONL file.
    """
    try:
        with open(file_path, "a") as f:
            f.write(json.dumps(data) + "\n")

        logger.info(f"Appended JSONL record ‚Üí {file_path}")

    except Exception as e:
        logger.error(f"Error writing JSONL to {file_path}: {e}+\n{traceback.format_exc()}")
        
def load_json_from_file(file_path):
    global inventory_list
    """
    Loads JSON data from a file and returns it as a Python object.

    Args:
        file_path: Path to the JSON file to read
    """
    try:
        with open(file_path, "r") as f:
            inventory_list = json.load(f)
            return inventory_list
    except Exception as e:
        logger.error(f"Pipeline Script - Error loading {file_path} file: %s", str(e)+"\n"+traceback.format_exc())
        return f"ü§ñ Agent: ‚ùå Failed - Could not load {file_path}"
    
        
# ============================================================================
# WORKER THREADS
# ============================================================================

def vlm_enhancer_consumer():
    """Consumer thread that processes VLM enhancement requests"""
    logger.info("Pipeline Script - [vlm_enhancer_consumer] Started")
    try:
        while True:
            payload = vlm_queue.get()
            if isinstance(payload, str):
                payload = json.loads(payload)
            
            if payload and "msg_type" in payload and payload["msg_type"] == "STREAM_END":
                logger.info("Pipeline Script - VLM Consumer received end of stream signal")
                result_queue.put(payload)
                break
            
            if payload and "data" in payload and len(payload["data"]) > 0:
                data = payload["data"]
                logger.info("Pipeline Script - Calling VLM with payload: %s", data)
                start_time = time.time()
                logger.info("‚è≥ [%s] Waiting for VLM call to finish ===", 
                            time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(start_time)))
                
                valid,result,err_msg = call_vlm(data, use_case=data.get("use_case", ""))
                logger.info("Pipeline Script - VLM Result: %s", result)
                payload["data"] = {"result": result, "valid": valid, "error": err_msg}
                result_queue.put(payload)
            vlm_queue.task_done()
            
            #print("‚è≥ Waiting for VLM call to finish")
    except Exception as e:
        logger.error("Pipeline Script - VLM Enhancer Consumer Error: %s", str(e))


# ============================================================================
# DATA STREAM READERS
# ============================================================================

def read_object_detection_stream():
    """Generator to read object detection messages from queue"""
    while True:
        while not od_message_queue.empty():
            item = od_message_queue.get(timeout=15)
            yield item
        time.sleep(0.1)


def read_vlm_results_stream():
    """Generator to read VLM results from queue"""
    while True:
        while not result_queue.empty():
            item = result_queue.get()
            yield item
        time.sleep(0.1)


# ============================================================================
# OBJECT DETECTION PIPELINE
# ============================================================================

def process_object_detection_results(video_file, use_case):
    """Process object detection results and prepare for VLM enhancement"""
    global result_queue, vlm_queue, inventory_list,inventory_set
    if not inventory_list:
        inventory_list = load_json_from_file(INVENTORY_FILE)
        inventory_set = set(item.strip().lower() for item in inventory_list)
    if video_file is None:
        logger.error("Pipeline Script - No video file provided for processing")
        yield "üìπ Object Detection: ‚ùå Failed - No video uploaded", {}
        return
    
    try:
        # Process streaming results
        best_frames = defaultdict(dict)
        ui_items = []
        for payload in read_object_detection_stream():
            try:
                if isinstance(payload, str):
                    payload = json.loads(payload)
                
                if payload and "msg_type" in payload and payload["msg_type"] == "STREAM_END":
                    logger.info("Pipeline Script - Object Detection stream ended")
                    break
                
                if not payload or not "data" in payload or not len(payload["data"]) > 0:
                    continue
                data = payload["data"]
                item = data.get("item_name")
                frame_names = data.get("frames", [])
                #print("\n\nDATA:",data)
                
                if item in  inventory_set:
                    print(f"‚úÖ Item found {BOLD}{CYAN}{item}{RESET} in inventory, ‚ùå skipping VLM call and best frame selection call")
                    logger.info("Pipeline Script - Item '%s' found in inventory, skipping VLM", item)
                    ui_items.append({"item_name":item,"match":True})
                    result_queue.put({"item_name": item})
                    continue
                if item in best_frames:
                    continue
                import time

                start_time = time.time()   

                # compute time to get best frame
                best_frame, score = get_best_frame(frame_names, bucket_name=data.get("bucket", ""))
                end_time = time.time()

                execution_time = end_time - start_time
                print(f"üèÜ Best frame for {BOLD}{CYAN}{item}{RESET}: {os.path.basename(best_frame)} | Stability score: {score:.4f}, | Execution Time: {execution_time:.4f} seconds")

                presigned_url = get_presigned_url(best_frame, bucket_name=data.get("bucket", ""))
                
                if not presigned_url:
                    logger.warning("Pipeline Script - Could not generate presigned URL for frame: %s", best_frame)
                    continue
                
                best_frames[item] = {
                    "best_frame": presigned_url,
                    "stability_score": score
                }
                item_rec = {"item_name":item,"match":False}
                ui_items.append(item_rec)
                enhancer_payload = {"presigned_url": presigned_url, "use_case": use_case}
                payload["data"] = enhancer_payload

                vlm_queue.put(payload)
                logger.info("Pipeline Script - Sent to VLM queue: %s", enhancer_payload)

        
            except Exception as e:
                logger.error("Pipeline Script - Error processing OD payload: %s", str(e))
                continue
        write_json_to_file({"od_results": ui_items}, COMMON_RESULTS_DIR_FULL_PATH)
        yield "üìπ Object Detection: ‚úÖ Completed", {"od_results": ui_items}

    except Exception as e:
        logger.error("Pipeline Script - Error in object detection processing: %s", str(e))
        yield f"üìπ Object Detection: ‚ùå Failed - {str(e)}", {}


# ============================================================================
# VLM ENHANCEMENT PIPELINE
# ============================================================================

def process_vlm_enhancement():
    """Process VLM enhancement results from the result queue"""
    global inventory_list, inventory_set
    if not inventory_list:
        inventory_list = load_json_from_file(INVENTORY_FILE)
        inventory_set = set(item.strip().lower() for item in inventory_list)
    try:
        for payload in read_vlm_results_stream():
            if isinstance(payload, str):
                    payload = json.loads(payload)
            if payload and "msg_type" in payload and payload["msg_type"] == "STREAM_END":
                logger.info("Pipeline Script - VLM enhancement stream ended")
                break
            
            if payload and "data" in payload and len(payload["data"]) > 0:
                data = payload["data"]

                if "error" in data and data["error"]:
                    logger.error("Pipeline Script - VLM enhancement error: %s", data["error"])
                    yield "ü§ñ VLM Enhancement: ‚ùå Failed - " + data["error"], []
                    return
                final_result = data.get("result", [])
                if final_result and len(final_result)>0:
                    for result in final_result:
                        item_name = result.get("item_name","").strip().lower()
                        if item_name in inventory_set:
                            result["match"] = True
                        else:
                            result["match"] = False
                logger.info("Pipeline Script - VLM enhancement result: %s", final_result)
                yield "ü§ñ VLM Enhancement: ‚ö° Running", final_result
        time.sleep(0.5)  # Ensure all processing is done
        yield "ü§ñ VLM Enhancement: ‚úÖ Completed", []
    
    except Exception as e:
        logger.error("Pipeline Script - Error in VLM enhancement processing: %s", str(e))
        yield f"ü§ñ VLM Enhancement: ‚ùå Failed - {str(e)}", []
        return 


# ============================================================================
# MAIN PIPELINE ORCHESTRATION
# ============================================================================

def execute_loss_prevention_pipeline(video_file):
    """Main orchestration function for the entire pipeline"""
    try:
        if video_file is None:
            logger.error("Pipeline Script - No video file uploaded")
            yield "üìπ Object Detection: ‚ùå Failed - No video uploaded", {}, "ü§ñ VLM Enhancement: ‚è≥ Pending", [], "ü§ñ Agent: ‚è≥ Pending", []
            return
        
        # Phase 1: Object Detection
        od_completed = False
        final_od_status = ""
        final_od_results = {}
        final_vlm_results = []
        use_case = os.path.splitext(video_file)[0].lower()
        for od_status, od_results in process_object_detection_results(video_file, use_case):
            final_od_status = od_status
            final_od_results = od_results
            yield od_status, od_results, "ü§ñ VLM Enhancement: ‚è≥ Pending", [], "ü§ñ Agent: ‚è≥ Pending", []
            
            if "‚ùå Failed" in od_status:
                logger.error("Pipeline Script - Object detection failed, skipping VLM")
                yield od_status, od_results, "ü§ñ VLM Enhancement: ‚ùå Skipped", [], "ü§ñ Agent: ‚ùå Skipped", []
                return
            
            if "‚úÖ" in od_status and "Completed" in od_status:
                od_completed = True
                yield "üìπ Object Detection: ‚úÖ Completed", final_od_results, "ü§ñ VLM Enhancement: ‚ö° Running", [], "ü§ñ Agent: ‚è≥ Pending", []
        
        # Phase 2: VLM Enhancement
        vlm_queue.put({"msg_type": "STREAM_END", "data": {}})
        
        for vlm_status, vlm_results in process_vlm_enhancement():
            final_vlm_results.extend(vlm_results)
            unique_results = list({d['item_name']: d for d in final_vlm_results}.values())
            yield "üìπ Object Detection: ‚úÖ Completed", final_od_results, vlm_status, unique_results, "ü§ñ Agent: ‚è≥ Pending", []
        
        write_json_to_file({"vlm_results":unique_results}, COMMON_RESULTS_DIR_FULL_PATH)
        
        
        yield "üìπ Object Detection: ‚úÖ Completed", final_od_results, "ü§ñ VLM Enhancement: ‚úÖ Completed", unique_results, "ü§ñ Agent: ‚ö° Running", []
        
        # Phase 3: Agent Call for inventory validation
        agent_status, agent_results = agent_call(unique_results)
        write_json_to_file({"agent_results":agent_results}, COMMON_RESULTS_DIR_FULL_PATH)
        
        yield "üìπ Object Detection: ‚úÖ Completed", final_od_results, "ü§ñ VLM Enhancement: ‚úÖ Completed", unique_results, agent_status, agent_results
        
    except Exception as e:
        error_message = f"Pipeline Error: {str(e)}"
        logger.error("Pipeline Script - Critical error in pipeline: %s", error_message)
        logger.error(traceback.format_exc())
        yield f"üìπ Object Detection: ‚ùå Failed - {error_message}", {}, f"ü§ñ VLM Enhancement: ‚ùå Failed - {error_message}", [], f"ü§ñ Agent: ‚ùå Failed - {error_message}", []

# ============================================================================
# AGENT CALL FOR INVENTORY VALIDATION
# ============================================================================

def agent_call(vlm_results, use_case="decision_agent"):
    """
    Agent call function that validates items against inventory and calls VLM for unmatched items.
    
    Args:
        vlm_results: List of items from VLM enhancement
        use_case: The use case for the pipeline
    
    Returns:
        tuple: (status_message, updated_results)
    """
    global inventory_list, inventory_set
    
    try:
        logger.info("Pipeline Script - [agent_call] Starting inventory validation")
        
        if inventory_list is None:
            inventory_list = load_json_from_file(INVENTORY_FILE)
            inventory_set = set(item.strip().lower() for item in inventory_list)
        
        # Collect items that don't match inventory
        unmatched_items = []
        matched_items = []
        
        for result in vlm_results:
            item_name = result.get("item_name", "").strip().lower()
            
            if item_name in inventory_set:
                logger.info("Pipeline Script - [agent_call] Item '%s' found in inventory", item_name)
                matched_items.append(result)
            else:
                logger.info("Pipeline Script - [agent_call] Item '%s' NOT found in inventory, will validate with VLM", item_name)
                unmatched_items.append(result)
        
        # If no unmatched items, return early
        if not unmatched_items:
            logger.info("Pipeline Script - [agent_call] All items matched inventory, no VLM validation needed")
            return "ü§ñ Agent: ‚úÖ All items validated against inventory", vlm_results
        
        # Call VLM for unmatched items
        logger.info("Pipeline Script - [agent_call] Calling VLM to validate %d unmatched items", len(unmatched_items))
        
        # Prepare data for VLM call
        vlm_data = {
            "items": [item.get("item_name", "") for item in unmatched_items],
            "use_case": use_case
        }
        
        valid, vlm_validation_result, err_msg = call_vlm(vlm_data, use_case=use_case)
        
        if not valid or err_msg:
            logger.error("Pipeline Script - [agent_call] VLM validation failed: %s", err_msg)
            return f"ü§ñ Agent: ‚ùå VLM validation failed - {err_msg}", vlm_results
        
        # Update results based on VLM validation
        validated_results = matched_items.copy()
        
        if vlm_validation_result and isinstance(vlm_validation_result, list):
            validated_results.extend(vlm_validation_result)
        else:
            # If VLM didn't return proper results, keep unmatched items as is
            validated_results.extend(unmatched_items)
        logger.info("Pipeline Script - [agent_call] Agent validation completed. Matched: %d, Validated: %d", 
                    len(matched_items), len(validated_results))
        
        return "ü§ñ Agent: ‚úÖ Inventory validation completed", validated_results
        
    except Exception as e:
        logger.error("Pipeline Script - [agent_call] Error in agent call: %s", str(e))
        logger.error(traceback.format_exc())
        return f"ü§ñ Agent: ‚ùå Failed - {str(e)}", vlm_results
# ============================================================================
# INITIALIZATION
# ============================================================================

def wait_for_ov_models():
    """Wait for OV model directory to be ready before starting pipeline"""
    model_dir = "/app/ov-model/Qwen2.5-VL-7B-Instruct/int8"
    logger.info("Checking for OV model directory: %s", model_dir)
    
    while True:
        if os.path.exists(model_dir):
            # Check if directory has any content
            try:
                contents = os.listdir(model_dir)
                if contents:
                    logger.info("‚úÖ OV model directory found with %d items", len(contents))
                    return True
                else:
                    logger.warning("OV model directory exists but is empty, waiting...")
            except Exception as e:
                logger.warning("Could not list model directory: %s", str(e))
        else:
            logger.warning("OV model directory not found, waiting...")
        
        time.sleep(10)

def wait_for_vlm_service(max_retries=30, retry_delay=2):
    """Wait for VLM service to be ready before starting pipeline"""
    import requests
    
    vlm_host = os.environ.get("VLM_SERVICE_HOST", "vlm-service")
    vlm_port = os.environ.get("VLM_SERVICE_PORT", "8000")
    health_url = f"http://{vlm_host}:{vlm_port}/health"
    
    logger.info("Checking VLM service health at: %s", health_url)
    
    for attempt in range(max_retries):
        try:
            response = requests.get(health_url, timeout=5)
            if response.status_code == 200:
                logger.info("‚úÖ VLM service is ready")
                return True
            else:
                logger.warning("VLM service returned status %d, retrying...", response.status_code)
        except requests.exceptions.RequestException as e:
            logger.warning("VLM service not ready (attempt %d/%d): %s", attempt + 1, max_retries, str(e))
        
        time.sleep(retry_delay)
    
    logger.error("‚ùå VLM service did not become ready after %d attempts", max_retries)
    return False

# Initialize all queues and consumers
def init_pipeline_components():
    """Initialize all necessary components for the pipeline"""
    global vlm_queue, result_queue, od_message_queue
    
    # Wait for models and VLM service before starting
    wait_for_ov_models()
    
    # Object Detection Consumer
    od_consumer = ODConsumer(od_message_queue,RABBITMQ_USER,RABBITMQ_PASSWORD)
    od_consumer.start_consumer()
    
    # VLM Enhancer Consumer
    vlm_enhancer_thread = threading.Thread(target=vlm_enhancer_consumer)
    vlm_enhancer_thread.start()
    
    return od_consumer, vlm_enhancer_thread

od_consumer, vlm_enhancer_thread = init_pipeline_components()
od_results_shown, od_pipeline_status,vlm_pipeline_status = False, False,False
print("\n================ START OF PIPELINE RUN =================\n")

def main(video_file_name=None):
    """Main function to execute the loss prevention pipeline"""    
    if video_file_name is None:
        video_file_name = os.getenv("VIDEO_NAME", "three_lp_uses_cases_combined.mp4")
    
    global od_results_shown, od_pipeline_status, vlm_pipeline_status
    
    agent_pipeline_status = False
    
    for step in execute_loss_prevention_pipeline(video_file_name):
        od_status, od_results, vlm_status, vlm_results, agent_status, agent_results = step
        
        # Print status updates
        if "pending" not in od_status.replace(" ","").lower() and not od_pipeline_status:
            print(f"{od_status}")
            od_pipeline_status = True
        if "pending" not in vlm_status.replace(" ","").lower() and not vlm_pipeline_status:
            print(f"{vlm_status}")
            vlm_pipeline_status = True
        if "pending" not in agent_status.replace(" ","").lower() and not agent_pipeline_status:
            print(f"{agent_status}")
            agent_pipeline_status = True
        
        # Log the pipeline step
        logger.info("Pipeline Script - Pipeline Step: OD=%s, VLM=%s, Agent=%s", od_status, vlm_status, agent_status)
        
        # Check and display Object Detection completion
        if "‚úÖ" in od_status and "Completed" in od_status and not od_results_shown:
            logger.info("Pipeline Script - Object Detection COMPLETED")
            logger.info("Pipeline Script - Object Detection Results: %s", od_results)
            print(f"{od_status}")
            print(f"üìπ Object Detection Results:\n{json.dumps(od_results, indent=2)}\n\n")
            od_results_shown = True
        
        # Check and display VLM Enhancement completion
        if "‚úÖ" in vlm_status and "Completed" in vlm_status:
            logger.info("Pipeline Script - VLM Enhancement COMPLETED")
            logger.info("Pipeline Script - VLM Enhancement Results: %s", vlm_results)
            print(f"{vlm_status}")
            print(f"ü§ñ VLM Enhancement Results:\n{json.dumps(vlm_results, indent=2)}\n\n")
        
        # Check and display Agent completion
        if "‚úÖ" in agent_status and "completed" in agent_status.lower():
            logger.info("Pipeline Script - Agent COMPLETED")
            logger.info("Pipeline Script - Agent Results: %s", agent_results)
            print(f"{agent_status}")
            print(f"ü§ñ Agent Results:\n{json.dumps(agent_results, indent=2)}")
        
        # Break on any failures
        if "‚ùå" in od_status or "Failed" in od_status or "‚ùå" in vlm_status or "Failed" in vlm_status or "‚ùå" in agent_status or "Failed" in agent_status:
            logger.error(f"Pipeline Script - Pipeline failed at step: OD=%s, VLM=%s, Agent=%s", od_status, vlm_status, agent_status)
            break
    
    return od_status, od_results, vlm_status, vlm_results, agent_status, agent_results


if __name__ == "__main__":
    
    result = main() 
    print(f"\n{'='*60}\nFinal Pipeline Results:\n{'='*60}")
    
    # Check if pipeline was successful
    od_status, od_results, vlm_status, vlm_results, agent_status, agent_results = result
    
    pipeline_successful = (
        "‚úÖ" in od_status and "Completed" in od_status and
        "‚úÖ" in vlm_status and "Completed" in vlm_status and
        "‚úÖ" in agent_status and "completed" in agent_status.lower()
    )
    
    if pipeline_successful:
        print(f"‚úÖ Pipeline executed successfully!\n")
        print(f"OD Status: {od_status}")
        print(f"OD Results: {json.dumps(od_results, indent=2)}")
        print(f"\nVLM Status: {vlm_status}")
        print(f"VLM Results: {json.dumps(vlm_results, indent=2)}")
        print(f"\nAgent Status: {agent_status}")
        print(f"Agent Results: {json.dumps(agent_results, indent=2)}")
    else:
        print(f"‚ùå Pipeline execution failed or incomplete\n")
        print(f"OD Status: {od_status}")
        print(f"VLM Status: {vlm_status}")
        print(f"Agent Status: {agent_status}")

vlm_enhancer_thread.join() 
logger.info("=== VLM Enhancer finished ===")
logger.info("=== END OF PIPELINE RUN ===\n\n\n")



